#!/usr/bin/env python3
"""
HTML Downloader
ä¸‹è¼‰æŒ‡å®šç¶²å€çš„ HTML å…§å®¹ä¸¦å„²å­˜åˆ°æœ¬åœ°æª”æ¡ˆ
"""

import os
import requests
import urllib.parse
from pathlib import Path
import time
from typing import List, Optional
import hashlib


class HTMLDownloader:
    def __init__(self, save_dir: str = "./site_raw_html", delay: float = 1.0):
        """
        åˆå§‹åŒ– HTML ä¸‹è¼‰å™¨
        
        Args:
            save_dir: å„²å­˜ç›®éŒ„è·¯å¾‘
            delay: è«‹æ±‚é–“éš”æ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.save_dir = Path(save_dir)
        self.delay = delay
        self.session = requests.Session()
        
        # è¨­ç½® User-Agent é¿å…è¢«ç¶²ç«™å°é–
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # å‰µå»ºå„²å­˜ç›®éŒ„
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
    def generate_filename(self, url: str) -> str:
        """
        æ ¹æ“š URL ç”Ÿæˆå®‰å…¨çš„æª”æ¡ˆåç¨±
        
        Args:
            url: ç¶²å€
            
        Returns:
            å®‰å…¨çš„æª”æ¡ˆåç¨±
        """
        # è§£æ URL
        parsed = urllib.parse.urlparse(url)
        domain = parsed.netloc
        path = parsed.path.strip('/')
        
        # å¦‚æœè·¯å¾‘ç‚ºç©ºï¼Œä½¿ç”¨ index
        if not path:
            path = "index"
        
        # æ›¿æ›ä¸å®‰å…¨çš„å­—ç¬¦
        safe_path = path.replace('/', '_').replace('\\', '_').replace(':', '_')
        safe_path = ''.join(c for c in safe_path if c.isalnum() or c in '._-')
        
        # å¦‚æœè·¯å¾‘å¤ªé•·ï¼Œä½¿ç”¨ hash
        if len(safe_path) > 50:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            safe_path = f"{safe_path[:30]}_{url_hash}"
        
        # çµ„åˆåŸŸåå’Œè·¯å¾‘
        filename = f"{domain}_{safe_path}.html"
        
        return filename
    
    def download_html(self, url: str) -> Optional[str]:
        """
        ä¸‹è¼‰å–®å€‹ç¶²é çš„ HTML
        
        Args:
            url: è¦ä¸‹è¼‰çš„ç¶²å€
            
        Returns:
            HTML å…§å®¹ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        try:
            print(f"æ­£åœ¨ä¸‹è¼‰: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # å˜—è©¦å–å¾—æ­£ç¢ºçš„ç·¨ç¢¼
            if response.encoding:
                html_content = response.text
            else:
                # å¦‚æœç„¡æ³•æª¢æ¸¬ç·¨ç¢¼ï¼Œå˜—è©¦ UTF-8
                html_content = response.content.decode('utf-8', errors='ignore')
            
            print(f"âœ… æˆåŠŸä¸‹è¼‰: {url} ({len(html_content)} å­—å…ƒ)")
            return html_content
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ä¸‹è¼‰å¤±æ•—: {url} - {str(e)}")
            return None
        except Exception as e:
            print(f"âŒ è™•ç†éŒ¯èª¤: {url} - {str(e)}")
            return None
    
    def save_html(self, url: str, html_content: str) -> bool:
        """
        å„²å­˜ HTML å…§å®¹åˆ°æª”æ¡ˆ
        
        Args:
            url: åŸå§‹ç¶²å€
            html_content: HTML å…§å®¹
            
        Returns:
            æ˜¯å¦æˆåŠŸå„²å­˜
        """
        try:
            filename = self.generate_filename(url)
            filepath = self.save_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"<!-- Downloaded from: {url} -->\n")
                f.write(f"<!-- Download time: {time.strftime('%Y-%m-%d %H:%M:%S')} -->\n\n")
                f.write(html_content)
            
            print(f"ğŸ’¾ å·²å„²å­˜: {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {url} - {str(e)}")
            return False
    
    def download_urls(self, urls: List[str]) -> dict:
        """
        æ‰¹é‡ä¸‹è¼‰å¤šå€‹ç¶²å€çš„ HTML
        
        Args:
            urls: ç¶²å€åˆ—è¡¨
            
        Returns:
            ä¸‹è¼‰çµæœçµ±è¨ˆ
        """
        results = {
            'success': 0,
            'failed': 0,
            'total': len(urls),
            'failed_urls': []
        }
        
        print(f"é–‹å§‹ä¸‹è¼‰ {len(urls)} å€‹ç¶²å€...")
        print(f"å„²å­˜ç›®éŒ„: {self.save_dir.absolute()}")
        print("-" * 50)
        
        for i, url in enumerate(urls, 1):
            print(f"[{i}/{len(urls)}] ", end="")
            
            # ä¸‹è¼‰ HTML
            html_content = self.download_html(url)
            
            if html_content:
                # å„²å­˜åˆ°æª”æ¡ˆ
                if self.save_html(url, html_content):
                    results['success'] += 1
                else:
                    results['failed'] += 1
                    results['failed_urls'].append(url)
            else:
                results['failed'] += 1
                results['failed_urls'].append(url)
            
            # å»¶é²é¿å…è«‹æ±‚éæ–¼é »ç¹
            if i < len(urls):
                time.sleep(self.delay)
        
        print("-" * 50)
        print(f"ä¸‹è¼‰å®Œæˆï¼æˆåŠŸ: {results['success']}, å¤±æ•—: {results['failed']}")
        
        if results['failed_urls']:
            print("\nå¤±æ•—çš„ç¶²å€:")
            for url in results['failed_urls']:
                print(f"  - {url}")
        
        return results


def main():
    """ä¸»å‡½æ•¸ - ç¤ºä¾‹ç”¨æ³•"""
    
    # ç¤ºä¾‹ç¶²å€åˆ—è¡¨
    urls = [
        "https://apps.atlasacademy.io/",
        "https://apps.atlasacademy.io/db/",
        "https://api.atlasacademy.io/docs/",
        # æ‚¨å¯ä»¥åœ¨é€™è£¡æ·»åŠ æ›´å¤šç¶²å€
    ]
    
    # å‰µå»ºä¸‹è¼‰å™¨
    downloader = HTMLDownloader(
        save_dir="./site_raw_html",
        delay=1.0  # æ¯æ¬¡è«‹æ±‚é–“éš” 1 ç§’
    )
    
    # é–‹å§‹ä¸‹è¼‰
    results = downloader.download_urls(urls)
    
    return results


if __name__ == "__main__":
    main()
